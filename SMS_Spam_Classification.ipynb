{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tark4O2k66if"
      },
      "outputs": [],
      "source": [
        "from ast import increment_lineno\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "dataset = pd.read_csv(\"SMSSpamCollection\", sep='\\t', names=['label', 'message'])\n",
        "#dataset\n",
        "#dataset.info()\n",
        "#dataset.describe\n",
        "dataset['label'] = dataset['label'].map({'ham': 0, 'spam': 1})\n",
        "#dataset\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "plt.figure(figsize=(8,8))\n",
        "g = sns.countplot(x=\"label\", data=dataset)\n",
        "p = plt.title('Countplot for Spam vs Ham as imbalanced dataset')\n",
        "p = plt.xlabel('Is the SMS Spam?')\n",
        "p = plt.ylabel('Count')\n",
        "# handaling imbalanced dataset using Oversampling\n",
        "only_spam = dataset[dataset['label'] == 1]\n",
        "only_ham = dataset[dataset['label'] == 0]\n",
        "print(\"Number of Spam SMS: \",len(only_spam))\n",
        "print(\"Number of Ham SMS: \",len(only_ham))\n",
        "count = int((dataset.shape[0] - only_spam.shape[0])/only_spam.shape[0])\n",
        "print(\"Number of SMS to be added: \",count)\n",
        "for i in range(count-1):\n",
        "  dataset = pd.concat([dataset, only_spam])\n",
        "dataset\n",
        "plt.figure(figsize=(8,8))\n",
        "g = sns.countplot(x=\"label\", data=dataset)\n",
        "p = plt.title('Countplot for Spam vs Ham as balanced dataset')\n",
        "p = plt.xlabel('Is the SMS Spam?')\n",
        "p = plt.ylabel('Count')\n",
        "# reating new feature word_count\n",
        "dataset['word_count'] = dataset['message'].apply(lambda x: len(x.split()))\n",
        "dataset\n",
        "plt.figure(figsize=(12,6))\n",
        "#(1,1)\n",
        "plt.subplot(1,2,1)\n",
        "g = sns.histplot(dataset[dataset[\"label\"] == 0].word_count, kde=True)\n",
        "p = plt.title('Distribution of Word Count for Ham SMS')\n",
        "#(1,2)\n",
        "plt.subplot(1,2,2)\n",
        "g = sns.histplot(dataset[dataset[\"label\"] == 1].word_count, color=\"red\", kde=True)\n",
        "p = plt.title('Distribution of Word Count for Spam SMS')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "#Creating new feature of containing currency symbols\n",
        "def currency_present(dataset):\n",
        "  currency_symbols = ['€', '$', '¥', '£', '₹']\n",
        "  for symbol in currency_symbols:\n",
        "    if symbol in dataset:\n",
        "      return 1\n",
        "  return 0\n",
        "dataset['contains_currency_symbols'] = dataset['message'].apply(currency_present)\n",
        "dataset\n",
        "\n",
        "#countplt for contains_currency_symbols\n",
        "plt.figure(figsize=(8,8))\n",
        "g = sns.countplot(x=\"contains_currency_symbols\", data=dataset, hue=\"label\")\n",
        "p = plt.title('Countplot for contains_currency_symbols')\n",
        "p = plt.xlabel('Does SMS contains_currency_symbols')\n",
        "p = plt.ylabel('Count')\n",
        "p = plt.legend(['Ham', 'Spam'], loc=9)\n",
        "\n",
        "#Creating new feature of containing numbers\n",
        "def number(data):\n",
        "  for i in data:\n",
        "    if ord(i) >= 48 and ord(i) <= 57:\n",
        "      return 1\n",
        "  return 0\n",
        "dataset['contains_numbers'] = dataset['message'].apply(number)\n",
        "dataset\n",
        "\n",
        "# Countplot for containing numbers\n",
        "plt.figure(figsize=(8,8))\n",
        "g = sns.countplot(x=\"contains_numbers\", data=dataset, hue=\"label\")\n",
        "p = plt.title('Countplot for containing numbers')\n",
        "p = plt.xlabel('Does SMS contains_numbers')\n",
        "p = plt.ylabel('Count')\n",
        "p = plt.legend(['Ham', 'Spam'], loc=9)\n",
        "\n",
        "# Data Cleaning\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "corpus = []\n",
        "wnl = WordNetLemmatizer()\n",
        "\n",
        "for sms in list(dataset.message):\n",
        "  message = re.sub(pattern = '[^a-zA]', repl = ' ', string = sms) # Filtering out special characters and numbers\n",
        "  message = message.lower()\n",
        "  words = message.split() # Tokenizer\n",
        "  filtered_words = [word for word in words if word not in stopwords.words('english')]\n",
        "  lemmatized_words = [wnl.lemmatize(word) for word in filtered_words]\n",
        "  corpus.append(' '.join(lemmatized_words))\n",
        "\n",
        "#corpus\n",
        "# Creating the Bag of words model\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf = TfidfVectorizer(max_features=500)\n",
        "vectors = tfidf.fit_transform(corpus).toarray()\n",
        "feature_names = tfidf.get_feature_names_out()\n",
        "\n",
        "X  = pd.DataFrame(vectors, columns=feature_names)\n",
        "y = dataset['label']\n",
        "\n",
        "from sklearn.model_selection import cross_val_score, train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_test\n",
        "\n",
        "# Naive Bayes Model\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "mnb = MultinomialNB()\n",
        "cv = cross_val_score(mnb, X, y, scoring = 'f1', cv = 10)\n",
        "print(round(cv.mean(), 3))\n",
        "print(round(cv.std(),3))\n",
        "\n",
        "mnb.fit(X_train, y_train)\n",
        "y_pred = mnb.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n",
        "\n",
        "plt.figure(figsize=(8,8))\n",
        "axis_labels = ['Ham', 'Spam']\n",
        "g = sns.heatmap(data=cm, xticklabels=axis_labels, yticklabels=axis_labels, annot = True, fmt='g', cbar_kws={\"shrink\": 0.5}, cmap=\"Blues\")\n",
        "p = plt.title('Confusion Matrix of Multinomila Naive Bayes Model')\n",
        "p = plt.xlabel('Actual Values')\n",
        "p = plt.ylabel('Predicted values')\n",
        "\n",
        "\n",
        "# Now using decision tree\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "dt = DecisionTreeClassifier()\n",
        "cv1 = cross_val_score(dt, X, y, scoring = 'f1', cv = 10)\n",
        "print(round(cv1.mean(), 3))\n",
        "print(round(cv1.std(),3))\n",
        "\n",
        "dt.fit(X_train, y_train)\n",
        "y_pred1 = dt.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n",
        "cm = confusion_matrix(y_test, y_pred1)\n",
        "print(cm)\n",
        "\n",
        "plt.figure(figsize=(8,8))\n",
        "axis_labels = ['Ham', 'Spam']\n",
        "g = sns.heatmap(data=cm, xticklabels=axis_labels, yticklabels=axis_labels, annot = True, fmt='g', cbar_kws={\"shrink\": 0.5}, cmap=\"Blues\")\n",
        "p = plt.title('Confusion Matrix of Decision Tree Model')\n",
        "p = plt.xlabel('Actual Values')\n",
        "p = plt.ylabel('Predicted values')\n",
        "\n",
        "def predict_spam(sms):\n",
        "  message = re.sub(pattern = '[^a-zA]', repl = ' ', string = sms) # Filtering out special characters and numbers\n",
        "  message = message.lower()\n",
        "  words = message.split() # Tokenizer\n",
        "  filtered_words = [word for word in words if word not in stopwords.words('english')]\n",
        "  lemmatized_words = [wnl.lemmatize(word) for word in filtered_words]\n",
        "  message = ' '.join(lemmatized_words)\n",
        "  temp = tfidf.transform([message]).toarray()\n",
        "  return dt.predict(temp)\n",
        "\n",
        "# prediction 1 - Lottery text message\n",
        "sample_message = \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\"\n",
        "if predict_spam(sample_message):\n",
        "  print(\"This is a Spam message.\")\n",
        "else:\n",
        "  print(\"This is a Ham(normal) message\")\n"
      ]
    }
  ]
}